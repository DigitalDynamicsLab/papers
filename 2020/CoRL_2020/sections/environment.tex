%!TEX root = ../main.tex

\section{Chrono Simulation Environment}
\label{sec:simEnv}
The physics-based simulator used in conjunction with this work is called Chrono. It is actively developed, it is open source, and it is released under a permissive BSD3 license for unfettered use/change/distribution \cite{projectChronoGithub}. A full description of the simulation platform falls outside the scope of this document; for an overview, see \cite{chronoOverview2016}. Chrono provides support for multibody dynamics (multi-core), nonlinear finite element analysis (multi-core), fluid solid interaction (GPU), granular dynamics (GPU), terramechanics (multi-core/GPU/MPI), sensing (GPU), and the simulation of large collections of AVs running in one joint scenario (MPI). The hardware support is as follows: multi-core CPUs, GPU computing via CUDA, and distributed memory (clusters/supercomputers) via the Message Passing Interface (MPI) standard. The four Chrono components relevant herein are: Chrono::Engine, Chrono::Vehicle, terramechanics, and Chrono::Sensor. Chrono::Engine is the solver that advances the simulation in time. Chrono::Vehicle provides support for rapidly setting up and analyzing vehicles (tracked or wheeled) via a library of templates for vehicle components (suspensions, steering systems, tires, powertrains/diverlines, tracks, etc.) \cite{ChronoVehicle2019}. The terramechanics support comes in three flavors: expeditious and semi-empirical approaches \cite{alessandroSCM2019}, via continuum representations, and using the discrete element method (fully resolved granular terrain).

Sensing support in Chrono is provided as an additional module that sits on top of Chrono::Engine to provide measurement data from inside the simulation and virtual environment. Currently, there is support for RGB cameras, lidar, GPS, and IMU \cite{TR-2020-07}. The sensor module's purpose is to provide realistic data for training and testing autonomous controls. For GPS and IMU, ground truth data, queried from Chrono::Engine is augmented to introduce noise commonly found on accelerometers and gyroscopes \cite{shah2018airsim} as well as GPS receivers. For camera and lidar, the visual environment is ray traced using custom GPU kernels that model the acquisition process of the specific sensor. The ray-traced data is then augmented to introduce noise and distortion to model the true sensor output. All sensors are parameterized by their update frequencies, noise characteristics, and lag. 

Our lidar model augments ground-truth data with noise (based on the measurements of range, intensity, and angular precision) to produce the final point cloud. The lidar leverages ray-tracing to create a point cloud based on the visual scene. This, in combination with supersampling for beam divergence, allows Chrono::Sensor to generate high-fidelity point clouds of complex environments. The beam discretization model extends that proposed in \cite{goodin2018enabling} to allow a user-defined number of rays per lidar beam. By incorporating beam divergence, we can model multiple return modes and encountered objects. In addition to beam divergence, the ray-tracing method allows the temporal sampling of a scanning lidar to be based on modern motion blurring techniques resulting in realistic and continuous distortions that are not possible with large time steps in video gaming collision detection systems employed by other learning environments, e.g. \cite{shah2018airsim,carlaAVsim2017}.

The implemented camera simulator introduces lens and image sensors models to improve the realism of the data. The camera is parameterized based on the frequency, resolution, field of view, exposure time, and lag. Based on the exposure time, motion blur that accounts for object and camera movement is introduced. The camera lens model draws on work from \cite{tang2017precision} to allow for wide angle lenses. The noise model is based on modified version of the EMVA standard \cite{standard20101288}, which introduces intensity-dependent noise based on the image sensor characteristics. Additional components of the image signal processor (ISP) are in development since the ISP introduces additional sensing artifacts such as compression, demosaicking, and color correction.

The Python API of Chrono, known as \emph{PyChrono}, provides access to the vast majority of Chrono API from Python, including Chrono::Vehicle and Chrono::Sensor. This allows for a simulation to be directly interfaced to the Python API of popular ML frameworks. By using the SWIG wrapper \cite{Beazley96SWIG} to directly interface with the C++ binaries, minimal overhead is introduced when running a simulation from Python. As an example, large data from sensor simulations (such as RGB images or lidar) are cast to NumPy arrays without instantiating new memory by means of SWIG typemaps. 


