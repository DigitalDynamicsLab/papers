%!TEX root = ../main.tex

\section{Related Work}
\label{sec:relatedWork}

%This section provides a summary of the state of the art in four areas: simulation engines (\S\ref{subsec:dynamicsEngines}); sensing simulation (\S\ref{subsec:sensingSimulation}); techniques used for learning (\S\ref{subsec:learningTechniques}); and software environments for learning automation (\S\ref{subsec:learningTechniques}).

This section provides a summary of the state of the art in simulation environments (\S\ref{subsec:simEnvironments}), and learning techniques for autonomous navigation (\S\ref{subsec:learningTechniques}). The discussion of simulation environments is restricted to those commonly used for training reinforcement learning algorithms.

\subsection{Simulation Environments for Reinforcement Learning}
\label{subsec:simEnvironments}

Gazebo \cite{koenig2004design}, one of the most broadly used simulators in robotics, has been used for reinforcement learning by leveraging the open-source nature and tight integration with ROS. Gazebo exposes an environment that wraps multiple dynamics engines and sensors. However, it lacks specific support for vehicle modeling and deformable terrain for off-road scenarios.

Widely used for ``in-door robotics'' reinforcement learning, MuJoCo \cite{todorovMujoco2012} is a dynamics engine that supports URDF-based modeling. MuJoCo is not open-source and it does not support high-fidelity vehicle dynamics with deformable terrain interaction which can dominate many off-road maneuvers. The sensing support is also limited due to the simplicity of the noise models that can be applied to the sensors, most of which are restricted to dynamics-based interoceptive sensors. 

An open-source alternative to MuJoCo is PyBullet \cite{matas2018simtoreal}, which exposes many of the same features but also lacks sophisticated sensing support. PyBullet provides an interface to generate the specific sensor data desired by the user. The strengths of PyBullet are in rigid-body dynamics and ease of use, making it a convenient choice for simpler RL applications, but limiting for off-road navigation.

In a more vehicle-centric approach, CARLA \cite{carlaAVsim2017}, AirSim \cite{shah2018airsim}, and Torcs \cite{torcsRacingSimulation2020} all seek to allow vehicle simulation for training and testing of control algorithms. While these are vehicle-focused, they cannot perform off-road simulation and have limited sensor fidelity. Torcs, which is originally based on a racing game, provides limited support for sensing, allowing access to camera and simplified lidar, with dynamic information available directly from the physics engine. CARLA and AirSim are designed for on-road applications and support an array of sensors. The sensors include basic distortions and noise. Due to limited geometric fidelity and time-resolution of collision-based ray-casting, the sensor data is typically overly clean or has obvious discontinuities or modeling artifacts. None of these on-road simulation environments support complex off-road navigation.


%%%%%%%%%%%%%%%%%%%%%%%%%% SIMONE
\subsection{Learning techniques}
\label{subsec:learningTechniques}

Deep Reinforcement Learning (DRL) techniques have shown outstanding level of performance since their very introduction \cite{Mnih13}. Subsequently, DRL has been used in vision-based robotic manipulation tasks. Robots controlled by DRL-trained neural networks (NN) have been shown to solve complex tasks in unstructured environments with \cite{zhu2018reinforcement} or without \cite{LevineFDA15} the use of imitation learning. 
End-to-end RL approaches have also been successfully applied to on-road autonomous driving. One of the major challenges in this area is the gap between RGB images generated by simulators and real world camera images, that can cause the autonomous driving policies trained in simulation to perform poorly in the real world. This issues has been addressed in various ways, such as using synthesized realistic images \cite{YurongALDriving17} or tools to generate images directly from real-world sampling \cite{Amini2020RLDriving}.

Sensor fusion with DRL techniques have shown promising results in controlling small indoor robots with camera + LiDAR \cite{BohezVCVSD17,PatelCKK2017}. RL in conjunction with imitation learning have been used for off-road driving when the vehicle learned to run laps quickly  \cite{Pan2017OffRoadAV}. However, to the best of our knowledge, there has been no demonstration of an end-to-end, off-road driving policy capable of reaching a target position while avoiding randomly placed obstacles on deformable soil and hilly terrain. 
